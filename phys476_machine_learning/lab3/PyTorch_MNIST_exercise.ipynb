{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\"\"\"\n",
    "This jupyter notebook is based on using PyTorch to\n",
    "classify 28x28 grayscale image data into an integer from 0-9\n",
    "\"\"\"\n",
    "\n",
    "var = 0   # so jupyterlab doesnt output the string above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this cell is for importing the data from keras.datasets\n",
    "and then casting them to tensors for use with PyTorch\n",
    "\"\"\"\n",
    "\n",
    "  # load data from keras.datasets\n",
    "(train_in_unshaped, train_out_unencoded), (test_in_unshaped, test_out_unencoded) = mnist.load_data()\n",
    "\n",
    "''' uncomment to see raw data + plot of digit\n",
    "print(train_out_unencoded[2])\n",
    "print(train_in_unshaped[2])\n",
    "plt.imshow(train_in_unshaped[2], interpolation='nearest')\n",
    "'''\n",
    "\n",
    "  # reshape all 60,000 28x28 vectors into 1x784 vectors\n",
    "np_train_in = train_in_unshaped.reshape(60000, 784)\n",
    "np_test_in = test_in_unshaped.reshape(10000, 784)\n",
    "\n",
    "  # cast our boiz to make sure no errors occur\n",
    "\n",
    "  # convert to torch tensors\n",
    "train_in = torch.from_numpy(np_train_in).float() / 255\n",
    "train_out = torch.from_numpy(train_out_unencoded).long() / 255\n",
    "\n",
    "test_in = torch.from_numpy(np_test_in).float() / 255\n",
    "test_out = torch.from_numpy(test_out_unencoded).long() / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" network hyperparameters \"\"\"\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "Dim_in = 784  # input dimension of network: row vector consisting of flattened images\n",
    "Dim_hid = 400 # dimension of hidden layer\n",
    "Dim_out = 10  # output dimension of network: row vector consisting of one-hot-encoded numbers 0-9\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\"\"\" pytorch parameters \"\"\"\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")  # run on GPU? :O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" construct the net \"\"\"\n",
    "'''\n",
    "input_layer     -> 784 x 1   (each image is 784x1)\n",
    "hidden_layer_1  -> 400 x 1    \n",
    "output_layer    -> 10 x 1    (expecting one hot encoded output)\n",
    "'''\n",
    "\n",
    "nnet = torch.nn.Sequential(\n",
    "    torch.nn.Linear(Dim_in, Dim_hid),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(Dim_hid, Dim_out),\n",
    "    )\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias\n",
    "    \n",
    "def loss_fn(ypred, out):\n",
    "    CE = torch.nn.functional.cross_entropy\n",
    "    loss = CE(ypred, out)\n",
    "    return loss\n",
    "\n",
    "def accuracy(ypred, out):\n",
    "    preds = torch.argmax(ypred, dim=1)\n",
    "    accs = (preds == out).float()\n",
    "    return accs.mean()\n",
    "\n",
    "\n",
    "  # first randomly initialize weights, use autograd to backpropagate\n",
    "weight_layer1 = torch.randn(size=(Dim_in, Dim_hid), device=device, dtype=dtype, requires_grad=True) / np.sqrt(Dim_in)\n",
    "weight_layer2 = torch.randn(size=(Dim_hid, Dim_out), device=device, dtype=dtype, requires_grad=True) / np.sqrt(Dim_hid)\n",
    "\n",
    "\"\"\" train neural net for certain number of epochs \"\"\"\n",
    "\n",
    "\n",
    "for step in range(epochs):\n",
    "    # first, taking the training_in data and matrix multiply (.mm()) by the first set of weights\n",
    "    # next, our activation function takes the form of .clamp forcing all our negatives to be zero\n",
    "    # finally, matrix multiply by the second set of weights\n",
    "    y_pred = nnet(train_in)\n",
    "    \n",
    "    # now compute our loss function by taking the square sum of the difference between the answer and our guess\n",
    "    cat_loss = loss_fn(y_pred, train_out)\n",
    "    acc = accuracy(y_pred, train_out)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(\"Epoch: {}, Loss: {:.2f}, Accuracy: {:.3f}\".format(step, cat_loss.item(), acc.item()))\n",
    "    \n",
    "    # run autograd backpropagation\n",
    "    cat_loss.backward()\n",
    "    \n",
    "    # manually update weights with gradient descent\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in nnet.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
